<p>Vendredi 19 juin 2025</p>

<table>
  <thead>
    <tr>
      <th>Horaire</th>
      <th>Orateur</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td>9h30-9h55</td>
      <td><strong>Acceuil</strong></td>
    </tr>
    <tr>
      <td>9h55-10h</td>
      <td><strong>Mot d’ouverture</strong></td>
    </tr>
    <tr>
      <td>10h-10h50</td>
      <td><strong>Antoine Collas</strong> -  Apprentissage automatique par optimisation Riemannienne</td>
    </tr>
    <tr>
      <td>10h50-11h30</td>
      <td><strong>Pause café + posters</strong></td>
    </tr>
    <tr>
      <td>11h30-12h20</td>
      <td><strong>Julie Digne</strong> - Apprentissage de représentation neuronale implicite.</td>
    </tr>
    <tr>
      <td>12h20-14h20</td>
      <td><strong>Buffet + posters</strong></td>
    </tr>
    <tr>
      <td>14h20-15h10</td>
      <td><strong>Rodolphe Le Riche</strong>  - Bayesian optimization with derivatives acceleration</td>
    </tr>
    <tr>
      <td>15h10-16h00</td>
      <td><strong>Romain Vo</strong> - Méthodes d’apprentissage pour la tomographie par rayons X</td>
    </tr>
  </tbody>
</table>

<p><br /></p>

<div class="speaker-container">
    <div class="speaker-name">Rodolphe Le Riche </div>
    <div class="speaker-title">Bayesian optimization with derivatives acceleration</div>
    
    <div class="speaker-abstract">
Bayesian optimization algorithms form an important class of methods to minimize functions that are costly to evaluate, which is a very common situation. These algorithms iteratively infer Gaussian processes from past observations of the function and decide where new observations should be made through the maximization of an acquisition criterion. Often, in particular in engineering practice, the objective function is defined on a compact set such as in a hyper-rectangle of a d-dimensional real space, and the bounds are chosen wide enough so that the optimum is inside the search domain. In this situation, this work provides a way to integrate in the acquisition criterion the a priori information that these functions, once modeled as GP trajectories, should be evaluated at their minima, and not at any point as usual acquisition criteria do. We propose an adaptation of the widely used Expected Improvement acquisition criterion that accounts only for GP trajectories where the first order partial derivatives are zero and the Hessian matrix is positive definite. The new acquisition criterion keeps an analytical, computationally efficient, expression. This new acquisition criterion is found to improve Bayesian optimization on a test bed of functions made of Gaussian process trajectories in dimensions 2, 3 and 5. The addition of first and second order derivative information is particularly useful for multimodal functions.
This talk corresponds to the paper : Guillaume Perrin and Rodolphe Le Riche Bayesian optimization with derivatives acceleration, Transactions on Machine Learning Research, issn=2835-8856, aug. 2024.
	    </div>
</div>

<div class="speaker-container">
    <div class="speaker-name">Julie Digne</div>
    <div class="speaker-title">Apprentissage de représentation neuronale implicite.</div>
    
    <div class="speaker-abstract">
    </div>

</div>

<div class="speaker-container">
    <div class="speaker-name">Antoine Collas</div>
    <div class="speaker-title"> Apprentissage automatique par optimisation Riemannienne</div>
    
    <div class="speaker-abstract">
	   
 </div>
</div>

<div class="speaker-container">
    <div class="speaker-name">Romain Vo</div>
    <div class="speaker-title"></div>
    
    <div class="speaker-abstract">
	    </div>
</div>

